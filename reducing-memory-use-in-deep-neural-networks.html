<!DOCTYPE html>
<html lang="en" data-bs-theme="dark">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="author" content="James W. Hanlon">
  <title>Reducing memory use in deep neural networks</title>
  <link rel="icon" type="image/png" sizes="32x32" href="./theme/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="./theme/images/favicon-16x16.png">
  <link rel="stylesheet" type="text/css" href="./theme/css/main.css"/>
  <link href="https://jameswhanlon.com/reeds/atom.xml"
        type="application/atom+xml" rel="alternate"
        title="James W. Hanlon Atom Feed" />
  <link href="https://jameswhanlon.com/reeds/rss.xml"
        type="application/rss+xml" rel="alternate"
        title="James W. Hanlon RSS Feed" />
  <!-- MathJax -->
  <script>
  MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
              svg: { fontCache: 'global' } };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
  <script src="./theme/js/bundle.js"></script>
  <script data-goatcounter="https://jameswhanlon.goatcounter.com/count"
          async src="//gc.zgo.at/count.js"></script>
</head>
<body>
  <header>
  <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
    <div class="container-fluid">
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav me-auto mb-2 mb-md-0 text-uppercase">
          <li class="nav-item">
              <a class="nav-link" href="/index.html">notes</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/projects.html">projects</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/archive.html">archive</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/about.html">about</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/links.html">links</a>
          </li>
        </ul>
        <a class="navbar-brand" href="#">James W. Hanlon</a>
        <button class="btn btn-secondary" type="button" onclick="toggleMode(this)">
          <i class="bi bi-brightness-high-fill"></i>
        </button>
      </div>
    </div>
  </nav>
  </header>

  <main class="flex-shrink-0">
  <div class="container">
  <h1>
    Reducing memory use in deep neural&nbsp;networks
  </h1>
  <div class="lead">
    <time class="published" datetime="2017-02-05T00:00:00+01:00">
      05 Feb 2017
    </time><br>
      <span class="article-tag small"><a href="/tag/computing.html">computing</a></span>
      <span class="article-tag small"><a href="/tag/machine-intelligence.html">machine-intelligence</a></span>
  </div>
  <div class="article-body">
    <p>The memory requirements for modern deep neural networks can be significant,
however memory on-chip is expensive relative to computational resources such as
integer and floating-point units, and access to external <span class="caps">DRAM</span> memory is orders
of magnitude slower. This article surveys some recent results that demonstrate
the economy of reducing memory use by reuse and&nbsp;re-computation.</p>
<p>Memory in neural networks is required to store input data, weight parameters,
and activations as an input propagates through the network. In training,
activations from an forward pass must be retained until they can be used to
calculate the error gradients in the backwards pass. A 50-layer ResNet network,
for example, has 25 million weight parameters and computes 16 million
activations in the forward pass. With a batch of 32, this data alone occupies 5
<span class="caps">GB</span>; additional memory is required to store the program&#8217;s instructions, input
data and temporary values, the last of which is multiplied by the level
of parallelism in the execution. Measuring the memory use of ResNet-50 training
on a Maxwell-generation Nvidia TitanX <span class="caps">GPU</span> shows that it uses up to 7.5 <span class="caps">GB</span> of
the 12 <span class="caps">GB</span>&nbsp;available.</p>
<p>Reducing memory use is beneficial for neural networks for several reasons.
First, it enables deeper networks to be trained, which have been shown to
deliver superior performance for specific tasks and generalisation to new
tasks. Second, it allows larger batch sizes to be used, which improves
throughput and parallelisation. And third, and perhaps most importantly, it
allows data to remain closer to where it is being operated on, reducing the
effects of longer latency and lower bandwidth of larger-capacity off-chip
memory, and consequently improving performance. To illustrate the challenge of
last point with modern <span class="caps">GPU</span> architectures, it has been observed that <a href="http://jmlr.org/proceedings/papers/v48/diamos16.pdf">the
Maxwell TitanX <span class="caps">GPU</span> processor cores have only 1 <span class="caps">KB</span> of memory that can be read
fast enough to saturate the floating-point
datapath</a>.</p>
<p>Two techniques to reduce memory use draw on the dataflow analysis that has been
developed over decades of work with compilers for sequential programming
languages. First, <a href="http://mxnet.io/architecture/note_memory.html#in-place-operations">operations such as activation functions can be performed in
place when the input data can be overwritten directly by the output, so the
memory is
reused</a>.
Second, memory can be reused by <a href="http://mxnet.io/architecture/note_memory.html#standard-memory-sharing">analysing the data dependencies between
operations in a network and allocating the same memory to operations that do
not use it
concurrently</a>.</p>
<p>The second approach is particularly effective when the entire neural network
can be analysed at compile time to create a fixed allocation of memory since
the runtime overheads of memory management reduce to almost zero. The
combination of these techniques have been shown <a href="https://arxiv.org/pdf/1604.06174v2.pdf">to reduce memory in neural
networks by a factor of two to three</a>.
These optimisation techniques are analogous to the dataflow in a sequential
program graph to allow the reuse of registers and stack memory, with their
relatively higher efficiency compared to dynamic memory allocation&nbsp;routines.</p>
<p>Another approach is to trade reduced memory for an increase in computation.
When the computational resources are underused, as they typically are in GPUs,
an increase in computation won’t necessarily increase runtime, and if it does,
can produce relatively higher savings of memory compared to the additional
computation. A simple technique in this vein is to discard values that are
relatively cheap to compute, such as activation functions, and re-compute them
when necessary. More substantial reductions can be achieved by discarding
retained activations in sets of consecutive layers of a network and re-computing
them when they are required during the backwards pass, from the closest set of
remaining activations. Recomputing activations over sets of layers has been
demonstrated by the <a href="https://mxnet.io">MXNet team</a> to deliver a factor-of-four
memory reduction for a ResNet-50 network, but more importantly, results in
memory use that scales sub-linearly with respect to the number of layers. The
team also demonstrated <a href="https://arxiv.org/pdf/1604.06174v2.pdf">training of a 1000-layer ResNet in under 12 <span class="caps">GB</span> on the
same Maxwell TitanX <span class="caps">GPU</span></a>.</p>
<p>A similar memory-reuse approach has been developed by researchers ar <a href="https://deepmind.com/">Google
DeepMind</a> with recurrent neural networks (RNNs). RNNs
are a special type of <span class="caps">DNN</span> that allows cycles in their structure to encode
behaviour over sequences of inputs.  For RNNs, <a href="https://arxiv.org/pdf/1606.03401v1.pdf">re-computation has been shown
to reduce memory by a factor of 20 for sequences of length 1000 with only a 30%
performance overhead</a>. The Baidu <a href="http://research.baidu.com/">Deep
Speech team</a> recently showed how they applied
various memory-saving techniques obtain a factor of 16 reduction in memory for
activations, enabling them to <a href="http://jmlr.org/proceedings/papers/v48/diamos16.pdf">train networks with 100 layers on a Maxwell
TitanX, when previously they could only train
9</a>.</p>
<p>Relative to memory, compute resources are cheap. The state-of-the-art results
surveyed show efficient use of memory through reuse and trading increased
computation for reduced memory use can deliver dramatic improvements in the
performance of neural networks. However, these results are for a processor with
very limited on-chip memory, just a few megabytes, and just <span class="caps">1KB</span> of fast memory
per core. A processor with a better balance between memory and compute,
allowing more of a neural network to be stored on-chip, may facilitate much
more dramatic&nbsp;improvements.</p>
<p>[An adapted version of this article first appeared on the <a href="https://www.graphcore.ai/blog/why-is-so-much-memory-needed-for-deep-neural-networks">Graphcore
blog</a>
and there was some discussion of it on <a href="https://news.ycombinator.com/item?id=13928523">Hacker
News</a>]</p>
  </div>
  <div class="article-footer">
    <p>Please get in touch (mail @ this domain) with any
    comments, corrections or suggestions.</p>
  </div>
  </div>
  </main>

  <hr>
  <footer class="text-muted">
    <div class="container">
      <div class="small">
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
          <img alt="Creative Commons Licence" style="border-width:0"
               src="https://i.creativecommons.org/l/by/4.0/80x15.png" />
        </a>
        <br>
        Unless otherwise noted, all content is freely available under a
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
          Creative Commons Attribution 4.0 International License</a>.<br>
        The views expressed on this website are the author’s personal views and should not be
        attributed to any other person, including that of their employer.<br>
        <br>
        Subscribe: <a href="https://jameswhanlon.com/reeds/atom.xml">Atom</a> /
        <a href="https://jameswhanlon.com/reeds/rss.xml">RSS</a>
      </div>
    </div>
  </footer>
  <script>
    function toggleMode(x) {
      if (document.documentElement.getAttribute('data-bs-theme') == 'dark') {
        document.documentElement.setAttribute('data-bs-theme','light')
      }
      else {
        document.documentElement.setAttribute('data-bs-theme','dark')
      }
      x.firstElementChild.classList.toggle('bi-brightness-high-fill');
      x.firstElementChild.classList.toggle('bi-moon-fill');
    }
  </script>
</body>
</html>